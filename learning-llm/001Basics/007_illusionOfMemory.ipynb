{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a4a50ca7-0e6d-4f81-a6d4-401d75594884",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6f89270b-25fd-4462-814f-810eaa26069d",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv(override=True)\n",
    "api_key = os.getenv('OPENAI_API_KEY')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f86847fb-bcd9-44e4-bb21-21c089bb0d4f",
   "metadata": {},
   "source": [
    "### this is what we have done till now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3ea24515-4d6b-442e-bb63-4e00cfb3f03f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hi Partha! Nice to meet you. How can I help you today? I can answer questions, help with writing or coding, plan things, explain concepts, or just chat. If you tell me your goal or topic, I’ll tailor my responses. For example, I can: explain something simply, draft an email, debug code, or help plan a trip. What would you like to start with?'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## sending a prompt containing my name in it\n",
    "messages1 = [{\"role\": \"system\" , \"content\": \"You are a helpful assistant\"},\n",
    "           {\"role\": \"user\" , \"content\": \"Hi my name is Partha\"}]\n",
    "#this is the format in which messages are sent. thus note that this messages is nothing but a list of dictionaries\n",
    "\n",
    "\n",
    "# creating an openAi object\n",
    "openai = OpenAI()\n",
    "\n",
    "# now making the call to openai using the chat-completions api as shown below \n",
    "response1 = openai.chat.completions.create(model=\"gpt-5-nano\", messages= messages1)\n",
    "response1.choices[0].message.content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd8d39e9-bae9-43ce-9eba-e22c538e0079",
   "metadata": {},
   "source": [
    "## now making another call and asking gpt my name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "999d5ec0-83e1-456b-b0c2-c21e045faf2a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I don’t know your name yet. What would you like me to call you? You can share your name or a nickname, and I’ll use it in this chat.'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages2 = [{\"role\": \"system\" , \"content\": \"You are a helpful assistant\"},\n",
    "           {\"role\": \"user\" , \"content\": \"what is my name?\"}]\n",
    "#this is the format in which messages are sent. thus note that this messages is nothing but a list of dictionaries\n",
    "\n",
    "\n",
    "# creating an openAi object\n",
    "openai = OpenAI()\n",
    "\n",
    "# now making the call to openai using the chat-completions api as shown below \n",
    "response2 = openai.chat.completions.create(model=\"gpt-5-nano\", messages= messages2)\n",
    "response2.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "801eb272-1fd7-4f07-bde1-32bea6189855",
   "metadata": {},
   "outputs": [],
   "source": [
    "# note that its not able to tell my name . since the chat does have the context , info about the previous chat. as each gpt interaction is stateless"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a565bab7-80cd-4881-8529-8bc74bc5fc03",
   "metadata": {},
   "source": [
    "### making a conversation . giving the illusion of memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "442e995f-6fe2-43fc-b576-f74f26ac84a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prompt message is : [{'role': 'system', 'content': 'You are a helpful assistant'}, {'role': 'user', 'content': 'Hi my name is Partha'}, {'role': 'assistant', 'content': 'Hi Partha! Nice to meet you. How can I help you today? I can answer questions, help with writing or coding, plan things, explain concepts, or just chat. If you tell me your goal or topic, I’ll tailor my responses. For example, I can: explain something simply, draft an email, debug code, or help plan a trip. What would you like to start with?'}, {'role': 'user', 'content': 'What is my name?'}] \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Partha. Would you like me to call you Partha or is there another name you prefer?'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# note how we are passing the response previously received as part of new prompt, to make it more like a conversation. also note the \"role= assistant\" using which we pass the response received earlier \n",
    "# to make it more line a conversation\n",
    "messages3 = [{\"role\": \"system\" , \"content\": \"You are a helpful assistant\"},\n",
    "             {\"role\": \"user\" , \"content\": \"Hi my name is Partha\"},\n",
    "            {\"role\":\"assistant\", \"content\": response1.choices[0].message.content},\n",
    "             {\"role\": \"user\" , \"content\": \"What is my name?\"},]\n",
    "\n",
    "print(f\"prompt message is : {messages3} \\n\")\n",
    "\n",
    "openai = OpenAI()\n",
    "\n",
    "response3 = openai.chat.completions.create(model=\"gpt-5-nano\", messages= messages3)\n",
    "response3.choices[0].message.content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4f6982f-56b6-41a3-b898-882b505d8f12",
   "metadata": {},
   "source": [
    "# To recap:\n",
    "\n",
    "1. every call to an llp is stateless\n",
    "2. we pass the entire conversation so far in the input prompt , everytime\n",
    "3. this gives the illusion that the LLM has memory - it apparently keeps the context of the conversation\n",
    "4. But this is a tric; it a by-product of providing the entire conversation , every time\n",
    "5. an LLM just predicts the most likely next token int the sequence\n",
    "\n",
    "\n",
    "the chatGpt product uses exactly this trick -every time we send a message, its the entire conversation that gets passed it.\n",
    "\n",
    "### does that mean we have to pay extra each time for all the conversation so far\n",
    "for sure it does.and that what we want . we want the llm to  predict the next toekns int he sequence , looking back on the entire conversation. We wan that compute to happen, so we need to electricity bill and the processing fee for it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "017b40bc-1412-4da1-aef6-7aa3c1a12386",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
